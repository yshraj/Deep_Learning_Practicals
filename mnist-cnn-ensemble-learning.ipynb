{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-01T19:21:23.914821Z","iopub.execute_input":"2022-03-01T19:21:23.915421Z","iopub.status.idle":"2022-03-01T19:21:23.942332Z","shell.execute_reply.started":"2022-03-01T19:21:23.915327Z","shell.execute_reply":"2022-03-01T19:21:23.941591Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Activation\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\n#from scipy.stats import mode\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:23:51.402323Z","iopub.execute_input":"2022-03-01T19:23:51.402871Z","iopub.status.idle":"2022-03-01T19:23:51.408318Z","shell.execute_reply.started":"2022-03-01T19:23:51.402818Z","shell.execute_reply":"2022-03-01T19:23:51.407247Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Read training and test data\nX_train_full=pd.read_csv('/kaggle/input/digit-recognizer/train.csv', header='infer').values\nX_test=pd.read_csv('/kaggle/input/digit-recognizer/test.csv', header='infer').values\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:21:30.878232Z","iopub.execute_input":"2022-03-01T19:21:30.878652Z","iopub.status.idle":"2022-03-01T19:21:35.755091Z","shell.execute_reply.started":"2022-03-01T19:21:30.878608Z","shell.execute_reply":"2022-03-01T19:21:35.754053Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:21:35.760397Z","iopub.execute_input":"2022-03-01T19:21:35.760722Z","iopub.status.idle":"2022-03-01T19:21:35.770540Z","shell.execute_reply.started":"2022-03-01T19:21:35.760684Z","shell.execute_reply":"2022-03-01T19:21:35.769902Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n#Separate label and images from the training data\nX_train=X_train_full[:,1:]\ny_train=X_train_full[:,0]\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:21:35.922231Z","iopub.execute_input":"2022-03-01T19:21:35.922479Z","iopub.status.idle":"2022-03-01T19:21:35.929695Z","shell.execute_reply.started":"2022-03-01T19:21:35.922447Z","shell.execute_reply":"2022-03-01T19:21:35.928876Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train__ = X_train.reshape(X_train.shape[0], 28, 28)\n\nfig, axis = plt.subplots(1, 4, figsize=(20, 10))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(X_train__[i], cmap='binary')\n    digit = y_train[i].argmax()\n    ax.set(title = f\"Real Number is {digit}\");\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:23:59.081123Z","iopub.execute_input":"2022-03-01T19:23:59.081385Z","iopub.status.idle":"2022-03-01T19:23:59.625207Z","shell.execute_reply.started":"2022-03-01T19:23:59.081355Z","shell.execute_reply":"2022-03-01T19:23:59.624514Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1440x720 with 4 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#Normalize train and test images\nX_train = (X_train.astype(np.float32) - 127.5)/127.5\nX_test = (X_test.astype(np.float32) - 127.5)/127.5","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:21:38.762009Z","iopub.execute_input":"2022-03-01T19:21:38.762724Z","iopub.status.idle":"2022-03-01T19:21:38.908438Z","shell.execute_reply.started":"2022-03-01T19:21:38.762685Z","shell.execute_reply":"2022-03-01T19:21:38.907688Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"del X_train_full\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:22:48.371404Z","iopub.execute_input":"2022-03-01T17:22:48.371829Z","iopub.status.idle":"2022-03-01T17:22:48.381507Z","shell.execute_reply.started":"2022-03-01T17:22:48.371784Z","shell.execute_reply":"2022-03-01T17:22:48.380374Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Reshpae train and test images from 784 to 28 x 28 x 1\nX_train=X_train.reshape(-1,28,28,1)\nX_test=X_test.reshape(-1,28,28,1)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:22:49.171193Z","iopub.execute_input":"2022-03-01T17:22:49.171567Z","iopub.status.idle":"2022-03-01T17:22:49.179278Z","shell.execute_reply.started":"2022-03-01T17:22:49.171536Z","shell.execute_reply":"2022-03-01T17:22:49.177605Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#One-hot encode class labels\ny_train_vectors=to_categorical(y_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:22:52.060468Z","iopub.execute_input":"2022-03-01T17:22:52.060794Z","iopub.status.idle":"2022-03-01T17:22:52.068538Z","shell.execute_reply.started":"2022-03-01T17:22:52.060761Z","shell.execute_reply":"2022-03-01T17:22:52.067474Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:22:52.449262Z","iopub.execute_input":"2022-03-01T17:22:52.449533Z","iopub.status.idle":"2022-03-01T17:22:52.456204Z","shell.execute_reply.started":"2022-03-01T17:22:52.449503Z","shell.execute_reply":"2022-03-01T17:22:52.454754Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(42000, 28, 28, 1)\n(28000, 28, 28, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val= train_test_split(X_train, y_train_vectors, test_size=0.2, random_state=2)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:22:54.240438Z","iopub.execute_input":"2022-03-01T17:22:54.241072Z","iopub.status.idle":"2022-03-01T17:22:54.596696Z","shell.execute_reply.started":"2022-03-01T17:22:54.241005Z","shell.execute_reply":"2022-03-01T17:22:54.595651Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def my_model3():\n    model=Sequential()\n    model.add( Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False, input_shape=(X_train.shape[1:])) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=80, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=96, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=112, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=144, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=160, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=176, kernel_size=(3,3), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(units=10))\n    model.add(BatchNormalization())\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:22:55.711876Z","iopub.execute_input":"2022-03-01T17:22:55.712841Z","iopub.status.idle":"2022-03-01T17:22:55.732096Z","shell.execute_reply.started":"2022-03-01T17:22:55.712802Z","shell.execute_reply":"2022-03-01T17:22:55.731141Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\ndef my_model5():\n    model=Sequential()\n    \n    model.add( Conv2D(filters=32, kernel_size=(5,5), strides=(1,1), padding='valid', activation=None, use_bias=False, input_shape=(X_train.shape[1:])) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=96, kernel_size=(5,5), strides=(1,1), padding='valid', activation=None, use_bias=False ) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=128, kernel_size=(5,5), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=160, kernel_size=(5,5), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n        \n    model.add(Flatten())\n    \n    model.add(Dense(units=10))\n    model.add(BatchNormalization())\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T18:05:37.499450Z","iopub.execute_input":"2022-03-01T18:05:37.499914Z","iopub.status.idle":"2022-03-01T18:05:37.522944Z","shell.execute_reply.started":"2022-03-01T18:05:37.499860Z","shell.execute_reply":"2022-03-01T18:05:37.521967Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#Define model with 7 x 7 valid convolution, kernel_size=3, stride 1, and ReLU activation. \n#Also use BatchNormalization\ndef my_model7():\n    model=Sequential()\n    \n    model.add( Conv2D(filters=48, kernel_size=(7,7), strides=(1,1), padding='valid', activation=None, use_bias=False, input_shape=(X_train.shape[1:])) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=96, kernel_size=(7,7), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=144, kernel_size=(7,7), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    \n    model.add( Conv2D(filters=192, kernel_size=(7,7), strides=(1,1), padding='valid', activation=None, use_bias=False) )\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n        \n    model.add(Flatten())\n    \n    model.add(Dense(units=10))\n    model.add(BatchNormalization())\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-01T18:05:37.884959Z","iopub.execute_input":"2022-03-01T18:05:37.885259Z","iopub.status.idle":"2022-03-01T18:05:37.897015Z","shell.execute_reply.started":"2022-03-01T18:05:37.885215Z","shell.execute_reply":"2022-03-01T18:05:37.895915Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#model3=my_model3()\nmodel5=my_model5()\nmodel7=my_model7()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T18:05:41.385991Z","iopub.execute_input":"2022-03-01T18:05:41.386716Z","iopub.status.idle":"2022-03-01T18:05:41.660420Z","shell.execute_reply.started":"2022-03-01T18:05:41.386661Z","shell.execute_reply":"2022-03-01T18:05:41.659235Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(featurewise_center=False,\n                             samplewise_center=False,\n                             featurewise_std_normalization=False,\n                             samplewise_std_normalization=False,\n                             zca_whitening=False,\n                             rotation_range=10,\n                             zoom_range=0.1,\n                             width_shift_range=0.1,\n                             height_shift_range=0.1,\n                             horizontal_flip=False,\n                             vertical_flip=False\n                            )","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:23:10.480298Z","iopub.execute_input":"2022-03-01T17:23:10.480789Z","iopub.status.idle":"2022-03-01T17:23:10.487176Z","shell.execute_reply.started":"2022-03-01T17:23:10.480756Z","shell.execute_reply":"2022-03-01T17:23:10.485886Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow(X_train, y_train,\n                                     batch_size=120,\n                                     shuffle=True)\n\nval_datagen = ImageDataGenerator()\nval_generator = val_datagen.flow(X_val, y_val,\n                                 batch_size=120,\n                                 shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:23:11.553278Z","iopub.execute_input":"2022-03-01T17:23:11.554313Z","iopub.status.idle":"2022-03-01T17:23:11.939543Z","shell.execute_reply.started":"2022-03-01T17:23:11.554272Z","shell.execute_reply":"2022-03-01T17:23:11.938571Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\n#Set how we plan to reduce learning rate on plateau\nreduceLROnPlateau = ReduceLROnPlateau(monitor='val_acc', \n                                patience=3,\n                                verbose=1, \n                                factor=0.5,\n                                min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:23:12.687198Z","iopub.execute_input":"2022-03-01T17:23:12.687482Z","iopub.status.idle":"2022-03-01T17:23:12.695370Z","shell.execute_reply.started":"2022-03-01T17:23:12.687451Z","shell.execute_reply":"2022-03-01T17:23:12.694184Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#fit 3 CNNs\nmodel3.fit(train_generator, epochs=150, callbacks=[reduceLROnPlateau], validation_data=val_generator)\n#model5.fit(train_generator, epochs=150, callbacks=[reduceLROnPlateau], validation_data=val_generator)\n#model7.fit(train_generator, epochs=150, callbacks=[reduceLROnPlateau], validation_data=val_generator)\n\n#Use 3-trained CNNs to make predictions. \n#Each prediction varialbe is a matrix of size 28K x 10 as there are 10 classes\nprediction_vectors3=model3.predict(X_test)\n#prediction_vectors5=model5.predict(X_test)\n#prediction_vectors7=model7.predict(X_test)\n\nprint(prediction_vectors3.shape)\n#print(prediction_vectors5.shape)\n#print(prediction_vectors7.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T17:23:13.833156Z","iopub.execute_input":"2022-03-01T17:23:13.833803Z","iopub.status.idle":"2022-03-01T18:01:14.366507Z","shell.execute_reply.started":"2022-03-01T17:23:13.833768Z","shell.execute_reply":"2022-03-01T18:01:14.365661Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2022-03-01 17:23:13.970222: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/150\n","output_type":"stream"},{"name":"stderr","text":"2022-03-01 17:23:16.344409: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"280/280 [==============================] - 24s 55ms/step - loss: 0.4987 - accuracy: 0.9256 - val_loss: 0.5038 - val_accuracy: 0.9352\nEpoch 2/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.2218 - accuracy: 0.9800 - val_loss: 0.1913 - val_accuracy: 0.9854\nEpoch 3/150\n280/280 [==============================] - 16s 57ms/step - loss: 0.1546 - accuracy: 0.9836 - val_loss: 0.2231 - val_accuracy: 0.9567\nEpoch 4/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.1185 - accuracy: 0.9859 - val_loss: 0.1038 - val_accuracy: 0.9892\nEpoch 5/150\n280/280 [==============================] - 16s 56ms/step - loss: 0.0984 - accuracy: 0.9865 - val_loss: 0.1114 - val_accuracy: 0.9792 l\nEpoch 6/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0811 - accuracy: 0.9874 - val_loss: 0.0660 - val_accuracy: 0.9868\nEpoch 7/150\n280/280 [==============================] - 16s 56ms/step - loss: 0.0689 - accuracy: 0.9888 - val_loss: 0.0620 - val_accuracy: 0.9892\nEpoch 8/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0631 - accuracy: 0.9888 - val_loss: 0.0639 - val_accuracy: 0.9850\nEpoch 9/150\n280/280 [==============================] - 16s 56ms/step - loss: 0.0544 - accuracy: 0.9899 - val_loss: 0.0455 - val_accuracy: 0.9902\nEpoch 10/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0541 - accuracy: 0.9893 - val_loss: 0.0635 - val_accuracy: 0.9829\nEpoch 11/150\n280/280 [==============================] - 16s 57ms/step - loss: 0.0472 - accuracy: 0.9896 - val_loss: 0.0577 - val_accuracy: 0.9862 - loss: 0.0469 - accuracy: \nEpoch 12/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0431 - accuracy: 0.9912 - val_loss: 0.0506 - val_accuracy: 0.98949 - accuracy - ETA: 4s - loss: 0\nEpoch 13/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0426 - accuracy: 0.9905 - val_loss: 0.2484 - val_accuracy: 0.9315\nEpoch 14/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0368 - accuracy: 0.9921 - val_loss: 0.0644 - val_accuracy: 0.9829 loss: 0.0363 - ac\nEpoch 15/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0368 - accuracy: 0.9915 - val_loss: 0.0285 - val_accuracy: 0.9932\nEpoch 16/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0344 - accuracy: 0.9922 - val_loss: 0.0342 - val_accuracy: 0.9908\nEpoch 17/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0308 - accuracy: 0.9927 - val_loss: 0.0391 - val_accuracy: 0.9905\nEpoch 18/150\n280/280 [==============================] - 16s 57ms/step - loss: 0.0327 - accuracy: 0.9918 - val_loss: 0.0351 - val_accuracy: 0.9907\nEpoch 19/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0331 - accuracy: 0.9921 - val_loss: 0.0429 - val_accuracy: 0.9887\nEpoch 20/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0264 - accuracy: 0.9946 - val_loss: 0.0359 - val_accuracy: 0.9900\nEpoch 21/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0281 - accuracy: 0.9932 - val_loss: 0.0341 - val_accuracy: 0.9905\nEpoch 22/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0273 - accuracy: 0.9928 - val_loss: 0.0242 - val_accuracy: 0.9936\nEpoch 23/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0279 - accuracy: 0.9924 - val_loss: 0.0702 - val_accuracy: 0.9798\nEpoch 24/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0262 - accuracy: 0.9925 - val_loss: 0.0395 - val_accuracy: 0.9888\nEpoch 25/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0208 - accuracy: 0.9944 - val_loss: 0.0296 - val_accuracy: 0.9917\nEpoch 26/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0215 - accuracy: 0.9948 - val_loss: 0.1360 - val_accuracy: 0.96454  - ETA: 2s - loss: - ETA: 1s - l - ETA: 0s - loss: 0.0215 - accura\nEpoch 27/150\n280/280 [==============================] - 18s 66ms/step - loss: 0.0226 - accuracy: 0.9941 - val_loss: 0.0191 - val_accuracy: 0.9949\nEpoch 28/150\n280/280 [==============================] - 18s 64ms/step - loss: 0.0227 - accuracy: 0.9936 - val_loss: 0.0262 - val_accuracy: 0.9927\nEpoch 29/150\n280/280 [==============================] - 17s 61ms/step - loss: 0.0222 - accuracy: 0.9937 - val_loss: 0.0281 - val_accuracy: 0.9929\nEpoch 30/150\n280/280 [==============================] - 18s 66ms/step - loss: 0.0192 - accuracy: 0.9943 - val_loss: 0.0327 - val_accuracy: 0.9910\nEpoch 31/150\n280/280 [==============================] - 17s 61ms/step - loss: 0.0215 - accuracy: 0.9945 - val_loss: 0.0328 - val_accuracy: 0.9919\nEpoch 32/150\n280/280 [==============================] - 18s 64ms/step - loss: 0.0167 - accuracy: 0.9955 - val_loss: 0.0363 - val_accuracy: 0.9906\nEpoch 33/150\n280/280 [==============================] - 17s 62ms/step - loss: 0.0191 - accuracy: 0.9948 - val_loss: 0.0211 - val_accuracy: 0.9939\nEpoch 34/150\n280/280 [==============================] - 18s 65ms/step - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.0280 - val_accuracy: 0.9924\nEpoch 35/150\n280/280 [==============================] - 17s 61ms/step - loss: 0.0182 - accuracy: 0.9950 - val_loss: 0.0381 - val_accuracy: 0.9889\nEpoch 36/150\n280/280 [==============================] - 17s 62ms/step - loss: 0.0174 - accuracy: 0.9950 - val_loss: 0.0282 - val_accuracy: 0.9914\nEpoch 37/150\n280/280 [==============================] - 17s 62ms/step - loss: 0.0159 - accuracy: 0.9957 - val_loss: 0.0247 - val_accuracy: 0.9931\nEpoch 38/150\n280/280 [==============================] - 17s 60ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0220 - val_accuracy: 0.9935\nEpoch 39/150\n280/280 [==============================] - 16s 56ms/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.0327 - val_accuracy: 0.9902\nEpoch 40/150\n280/280 [==============================] - 17s 60ms/step - loss: 0.0175 - accuracy: 0.9955 - val_loss: 0.0227 - val_accuracy: 0.9937\nEpoch 41/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 0.0222 - val_accuracy: 0.9939\nEpoch 42/150\n280/280 [==============================] - 17s 59ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 0.0296 - val_accuracy: 0.9920\nEpoch 43/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0128 - accuracy: 0.9967 - val_loss: 0.0218 - val_accuracy: 0.9944\nEpoch 44/150\n280/280 [==============================] - 16s 59ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0175 - val_accuracy: 0.9945\nEpoch 45/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0121 - accuracy: 0.9965 - val_loss: 0.0262 - val_accuracy: 0.9919oss: 0.0122 - ac\nEpoch 46/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0116 - accuracy: 0.9966 - val_loss: 0.0296 - val_accuracy: 0.9923\nEpoch 47/150\n280/280 [==============================] - 14s 52ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.0411 - val_accuracy: 0.9880\nEpoch 48/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0112 - accuracy: 0.9973 - val_loss: 0.0206 - val_accuracy: 0.9940\nEpoch 49/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.0610 - val_accuracy: 0.9829\nEpoch 50/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0118 - accuracy: 0.9967 - val_loss: 0.0249 - val_accuracy: 0.9938\nEpoch 51/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0197 - val_accuracy: 0.9954\nEpoch 52/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.0188 - val_accuracy: 0.9948\nEpoch 53/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.0186 - val_accuracy: 0.9952\nEpoch 54/150\n280/280 [==============================] - 14s 52ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0208 - val_accuracy: 0.9938\nEpoch 55/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0115 - accuracy: 0.9968 - val_loss: 0.0220 - val_accuracy: 0.9935\nEpoch 56/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.0333 - val_accuracy: 0.9918\nEpoch 57/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0090 - accuracy: 0.9974 - val_loss: 0.0462 - val_accuracy: 0.9885\nEpoch 58/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.0196 - val_accuracy: 0.9936\nEpoch 59/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 0.0200 - val_accuracy: 0.9952\nEpoch 60/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0104 - accuracy: 0.9971 - val_loss: 0.0279 - val_accuracy: 0.9927\nEpoch 61/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0257 - val_accuracy: 0.9929\nEpoch 62/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.0218 - val_accuracy: 0.9935\nEpoch 63/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 0.0271 - val_accuracy: 0.9936\nEpoch 64/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.0571 - val_accuracy: 0.9862 loss: 0.0088 - accuracy\nEpoch 65/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0096 - accuracy: 0.9974 - val_loss: 0.0212 - val_accuracy: 0.9944\nEpoch 66/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0090 - accuracy: 0.9978 - val_loss: 0.0267 - val_accuracy: 0.9935\nEpoch 67/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.0234 - val_accuracy: 0.9939\nEpoch 68/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.0256 - val_accuracy: 0.9931\nEpoch 69/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0090 - accuracy: 0.9974 - val_loss: 0.0195 - val_accuracy: 0.9945\nEpoch 70/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0222 - val_accuracy: 0.9948\nEpoch 71/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0289 - val_accuracy: 0.9927\nEpoch 72/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0211 - val_accuracy: 0.9938073 - ac\nEpoch 73/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0184 - val_accuracy: 0.9948\nEpoch 74/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.0228 - val_accuracy: 0.9945\nEpoch 75/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 0.0247 - val_accuracy: 0.9931\nEpoch 76/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.0240 - val_accuracy: 0.9937\nEpoch 77/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0374 - val_accuracy: 0.9893\nEpoch 78/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0218 - val_accuracy: 0.9954\nEpoch 79/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0193 - val_accuracy: 0.9950\nEpoch 80/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.1655 - val_accuracy: 0.9548\nEpoch 81/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0258 - val_accuracy: 0.9942\nEpoch 82/150\n280/280 [==============================] - 16s 55ms/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0245 - val_accuracy: 0.9939\nEpoch 83/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0240 - val_accuracy: 0.9939\nEpoch 84/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0208 - val_accuracy: 0.9954\nEpoch 85/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0275 - val_accuracy: 0.9935\nEpoch 86/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0201 - val_accuracy: 0.9951\nEpoch 87/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0346 - val_accuracy: 0.9912\nEpoch 88/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.0255 - val_accuracy: 0.9938\nEpoch 89/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0213 - val_accuracy: 0.9952\nEpoch 90/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0217 - val_accuracy: 0.9946\nEpoch 91/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0220 - val_accuracy: 0.9946\nEpoch 92/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0231 - val_accuracy: 0.9939\nEpoch 93/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0288 - val_accuracy: 0.99401s - loss: 0.004 - ETA: 0s - loss: 0.0042 - accuracy: 0. - ETA: 0s - loss: 0.0041 - accuracy: 0.99 - ETA: 0s - loss: 0\nEpoch 94/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0215 - val_accuracy: 0.99551s - loss: 0.006 - ETA: 0s - los\nEpoch 95/150\n280/280 [==============================] - 16s 56ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0283 - val_accuracy: 0.9925\nEpoch 96/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0225 - val_accuracy: 0.9949TA: 12s - l - ETA: 3s - loss: 0.0061 - accuracy: 0. - ETA: 3s - loss: 0.0 - ETA: 0s - los\nEpoch 97/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0214 - val_accuracy: 0.9945\nEpoch 98/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.0233 - val_accuracy: 0.9945\nEpoch 99/150\n280/280 [==============================] - 16s 58ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0212 - val_accuracy: 0.9944\nEpoch 100/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0229 - val_accuracy: 0.9940\nEpoch 101/150\n280/280 [==============================] - 16s 57ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0253 - val_accuracy: 0.9938\nEpoch 102/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0186 - val_accuracy: 0.9951\nEpoch 103/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.9929\nEpoch 104/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0255 - val_accuracy: 0.9945\nEpoch 105/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0211 - val_accuracy: 0.9946\nEpoch 106/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0197 - val_accuracy: 0.9952\nEpoch 107/150\n280/280 [==============================] - 14s 52ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9915\nEpoch 108/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0207 - val_accuracy: 0.9943\nEpoch 109/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0222 - val_accuracy: 0.9951\nEpoch 110/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.0297 - val_accuracy: 0.9927\nEpoch 111/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0196 - val_accuracy: 0.9950\nEpoch 112/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0247 - val_accuracy: 0.9943\nEpoch 113/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0207 - val_accuracy: 0.9944\nEpoch 114/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0225 - val_accuracy: 0.9939\nEpoch 115/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0213 - val_accuracy: 0.9948\nEpoch 116/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0254 - val_accuracy: 0.9930\nEpoch 117/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0264 - val_accuracy: 0.9933\nEpoch 118/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0215 - val_accuracy: 0.9944\nEpoch 119/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0254 - val_accuracy: 0.9939\nEpoch 120/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0229 - val_accuracy: 0.9945\nEpoch 121/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0225 - val_accuracy: 0.9944\nEpoch 122/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0686 - val_accuracy: 0.9824\nEpoch 123/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0288 - val_accuracy: 0.9938\nEpoch 124/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0245 - val_accuracy: 0.9946\nEpoch 125/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0209 - val_accuracy: 0.9958\nEpoch 126/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0273 - val_accuracy: 0.9929\nEpoch 127/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0248 - val_accuracy: 0.9945\nEpoch 128/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0281 - val_accuracy: 0.9935\nEpoch 129/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0317 - val_accuracy: 0.9900\nEpoch 130/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0371 - val_accuracy: 0.9925\nEpoch 131/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0219 - val_accuracy: 0.9948\nEpoch 132/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0236 - val_accuracy: 0.9944\nEpoch 133/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0270 - val_accuracy: 0.9943\nEpoch 134/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0239 - val_accuracy: 0.9948\nEpoch 135/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0253 - val_accuracy: 0.9943\nEpoch 136/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0243 - val_accuracy: 0.9935\nEpoch 137/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0227 - val_accuracy: 0.9946\nEpoch 138/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0254 - val_accuracy: 0.9939\nEpoch 139/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0248 - val_accuracy: 0.9944\nEpoch 140/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0205 - val_accuracy: 0.9950\nEpoch 141/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0241 - val_accuracy: 0.9944\nEpoch 142/150\n280/280 [==============================] - 15s 55ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0208 - val_accuracy: 0.9948\nEpoch 143/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0310 - val_accuracy: 0.9933\nEpoch 144/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0236 - val_accuracy: 0.9946\nEpoch 145/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0195 - val_accuracy: 0.9956\nEpoch 146/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0202 - val_accuracy: 0.9951\nEpoch 147/150\n280/280 [==============================] - 15s 54ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0359 - val_accuracy: 0.9930\nEpoch 148/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0282 - val_accuracy: 0.9950\nEpoch 149/150\n280/280 [==============================] - 15s 53ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0205 - val_accuracy: 0.9952\nEpoch 150/150\n280/280 [==============================] - 15s 52ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0196 - val_accuracy: 0.9958\n(28000, 10)\n","output_type":"stream"}]},{"cell_type":"code","source":"#fit 3 CNNs\n#model3.fit(train_generator, epochs=150, callbacks=[reduceLROnPlateau], validation_data=val_generator)\nmodel5.fit(train_generator, epochs=150, callbacks=[reduceLROnPlateau], validation_data=val_generator)\n#model7.fit(train_generator, epochs=150, callbacks=[reduceLROnPlateau], validation_data=val_generator)\n\n#Use 3-trained CNNs to make predictions. \n#Each prediction varialbe is a matrix of size 28K x 10 as there are 10 classes\n#prediction_vectors3=model3.predict(X_test)\nprediction_vectors5=model5.predict(X_test)\n#prediction_vectors7=model7.predict(X_test)\n\n#print(prediction_vectors3.shape)\n#print(prediction_vectors5.shape)\n#print(prediction_vectors7.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T18:05:55.497022Z","iopub.execute_input":"2022-03-01T18:05:55.497364Z","iopub.status.idle":"2022-03-01T19:13:39.025682Z","shell.execute_reply.started":"2022-03-01T18:05:55.497314Z","shell.execute_reply":"2022-03-01T19:13:39.024652Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/150\n280/280 [==============================] - 16s 51ms/step - loss: 0.4671 - accuracy: 0.9391 - val_loss: 0.4585 - val_accuracy: 0.9235\nEpoch 2/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.2204 - accuracy: 0.9829 - val_loss: 0.1803 - val_accuracy: 0.9863\nEpoch 3/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.1532 - accuracy: 0.9854 - val_loss: 0.1163 - val_accuracy: 0.9905\nEpoch 4/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.1121 - accuracy: 0.9881 - val_loss: 0.0840 - val_accuracy: 0.9915\nEpoch 5/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0916 - accuracy: 0.9891 - val_loss: 0.0797 - val_accuracy: 0.9899\nEpoch 6/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0759 - accuracy: 0.9896 - val_loss: 0.0759 - val_accuracy: 0.9904\nEpoch 7/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0649 - accuracy: 0.9902 - val_loss: 0.0625 - val_accuracy: 0.9912\nEpoch 8/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0558 - accuracy: 0.9913 - val_loss: 0.0682 - val_accuracy: 0.9920\nEpoch 9/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0517 - accuracy: 0.9916 - val_loss: 0.0415 - val_accuracy: 0.9920\nEpoch 10/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0467 - accuracy: 0.9917 - val_loss: 0.0685 - val_accuracy: 0.9850\nEpoch 11/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0410 - accuracy: 0.9930 - val_loss: 0.0543 - val_accuracy: 0.9908\nEpoch 12/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0378 - accuracy: 0.9925 - val_loss: 0.0264 - val_accuracy: 0.9926\nEpoch 13/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0367 - accuracy: 0.9929 - val_loss: 0.0500 - val_accuracy: 0.9886\nEpoch 14/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0330 - accuracy: 0.9932 - val_loss: 0.0398 - val_accuracy: 0.9908\nEpoch 15/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0291 - accuracy: 0.9939 - val_loss: 0.0253 - val_accuracy: 0.9937\nEpoch 16/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0277 - accuracy: 0.9940 - val_loss: 0.3111 - val_accuracy: 0.9194\nEpoch 17/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0304 - accuracy: 0.9925 - val_loss: 0.0253 - val_accuracy: 0.9933\nEpoch 18/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0289 - accuracy: 0.9928 - val_loss: 0.0221 - val_accuracy: 0.9951\nEpoch 19/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0234 - accuracy: 0.9947 - val_loss: 0.0246 - val_accuracy: 0.9935\nEpoch 20/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0235 - accuracy: 0.9948 - val_loss: 0.0262 - val_accuracy: 0.9926\nEpoch 21/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0250 - accuracy: 0.9939 - val_loss: 0.0274 - val_accuracy: 0.9930\nEpoch 22/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0214 - accuracy: 0.9948 - val_loss: 0.0240 - val_accuracy: 0.9942\nEpoch 23/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.0321 - val_accuracy: 0.9915\nEpoch 24/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0204 - accuracy: 0.9948 - val_loss: 0.0380 - val_accuracy: 0.9904\nEpoch 25/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0200 - accuracy: 0.9950 - val_loss: 0.0270 - val_accuracy: 0.9930\nEpoch 26/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0183 - accuracy: 0.9952 - val_loss: 0.0406 - val_accuracy: 0.9892\nEpoch 27/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0183 - accuracy: 0.9950 - val_loss: 0.0324 - val_accuracy: 0.9902\nEpoch 28/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.0260 - val_accuracy: 0.9927\nEpoch 29/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0149 - accuracy: 0.9962 - val_loss: 0.0214 - val_accuracy: 0.9940\nEpoch 30/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0165 - accuracy: 0.9958 - val_loss: 0.0258 - val_accuracy: 0.9931\nEpoch 31/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0148 - accuracy: 0.9963 - val_loss: 0.0307 - val_accuracy: 0.9914\nEpoch 32/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0117 - accuracy: 0.9970 - val_loss: 0.0311 - val_accuracy: 0.9921\nEpoch 33/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.0290 - val_accuracy: 0.9930\nEpoch 34/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.0229 - val_accuracy: 0.9951\nEpoch 35/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0120 - accuracy: 0.9966 - val_loss: 0.0223 - val_accuracy: 0.9942\nEpoch 36/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.0220 - val_accuracy: 0.9930\nEpoch 37/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0130 - accuracy: 0.9962 - val_loss: 0.0248 - val_accuracy: 0.9932\nEpoch 38/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0115 - accuracy: 0.9968 - val_loss: 0.0377 - val_accuracy: 0.9900\nEpoch 39/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0097 - accuracy: 0.9978 - val_loss: 0.0260 - val_accuracy: 0.9931\nEpoch 40/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0118 - accuracy: 0.9967 - val_loss: 0.0195 - val_accuracy: 0.9950\nEpoch 41/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0130 - accuracy: 0.9965 - val_loss: 0.0231 - val_accuracy: 0.9935\nEpoch 42/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0123 - accuracy: 0.9964 - val_loss: 0.0267 - val_accuracy: 0.9932\nEpoch 43/150\n280/280 [==============================] - 14s 51ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 0.0257 - val_accuracy: 0.9924\nEpoch 44/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 0.0239 - val_accuracy: 0.9940\nEpoch 45/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.0217 - val_accuracy: 0.9946\nEpoch 46/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0193 - val_accuracy: 0.9946\nEpoch 47/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.0289 - val_accuracy: 0.9929\nEpoch 48/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.0205 - val_accuracy: 0.9942\nEpoch 49/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.0189 - val_accuracy: 0.9951\nEpoch 50/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.0248 - val_accuracy: 0.9923\nEpoch 51/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.0216 - val_accuracy: 0.9944\nEpoch 52/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.0225 - val_accuracy: 0.9937\nEpoch 53/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0214 - val_accuracy: 0.9944\nEpoch 54/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0179 - val_accuracy: 0.9952\nEpoch 55/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.0259 - val_accuracy: 0.9933\nEpoch 56/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0210 - val_accuracy: 0.9944\nEpoch 57/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0221 - val_accuracy: 0.9938\nEpoch 58/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0300 - val_accuracy: 0.9932\nEpoch 59/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0191 - val_accuracy: 0.9949\nEpoch 60/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0290 - val_accuracy: 0.9931\nEpoch 61/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.0211 - val_accuracy: 0.9944\nEpoch 62/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.0190 - val_accuracy: 0.9950\nEpoch 63/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.0249 - val_accuracy: 0.9938\nEpoch 64/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0224 - val_accuracy: 0.9944\nEpoch 65/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0224 - val_accuracy: 0.9944\nEpoch 66/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0253 - val_accuracy: 0.9927\nEpoch 67/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0246 - val_accuracy: 0.9945\nEpoch 68/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.0248 - val_accuracy: 0.9933\nEpoch 69/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0189 - val_accuracy: 0.9952\nEpoch 70/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0203 - val_accuracy: 0.9949\nEpoch 71/150\n280/280 [==============================] - 12s 44ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.0231 - val_accuracy: 0.9942\nEpoch 72/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0290 - val_accuracy: 0.9936\nEpoch 73/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0241 - val_accuracy: 0.9940\nEpoch 74/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.0240 - val_accuracy: 0.9940\nEpoch 75/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0209 - val_accuracy: 0.9949\nEpoch 76/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0216 - val_accuracy: 0.9940\nEpoch 77/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0244 - val_accuracy: 0.9938\nEpoch 78/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0314 - val_accuracy: 0.9924\nEpoch 79/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0218 - val_accuracy: 0.9946\nEpoch 80/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0202 - val_accuracy: 0.9942\nEpoch 81/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0219 - val_accuracy: 0.9944\nEpoch 82/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0237 - val_accuracy: 0.9932\nEpoch 83/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0206 - val_accuracy: 0.9936\nEpoch 84/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0233 - val_accuracy: 0.9936\nEpoch 85/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0184 - val_accuracy: 0.9956\nEpoch 86/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0219 - val_accuracy: 0.9946\nEpoch 87/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0226 - val_accuracy: 0.9940\nEpoch 88/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0274 - val_accuracy: 0.9930\nEpoch 89/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0188 - val_accuracy: 0.9944\nEpoch 90/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0168 - val_accuracy: 0.9955\nEpoch 91/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0187 - val_accuracy: 0.9956\nEpoch 92/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0232 - val_accuracy: 0.9949\nEpoch 93/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0297 - val_accuracy: 0.9929\nEpoch 94/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0207 - val_accuracy: 0.9942\nEpoch 95/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0217 - val_accuracy: 0.9952\nEpoch 96/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0300 - val_accuracy: 0.9929\nEpoch 97/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0293 - val_accuracy: 0.9933\nEpoch 98/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0190 - val_accuracy: 0.9946\nEpoch 99/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0204 - val_accuracy: 0.9942\nEpoch 100/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0259 - val_accuracy: 0.9940\nEpoch 101/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0294 - val_accuracy: 0.9940\nEpoch 102/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0228 - val_accuracy: 0.9945\nEpoch 103/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0267 - val_accuracy: 0.9939\nEpoch 104/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0209 - val_accuracy: 0.9956\nEpoch 105/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0233 - val_accuracy: 0.9944\nEpoch 106/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0232 - val_accuracy: 0.9942\nEpoch 107/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0176 - val_accuracy: 0.9950\nEpoch 108/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0260 - val_accuracy: 0.9936\nEpoch 109/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0214 - val_accuracy: 0.9946\nEpoch 110/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0261 - val_accuracy: 0.9936\nEpoch 111/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0272 - val_accuracy: 0.9937\nEpoch 112/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0281 - val_accuracy: 0.9933\nEpoch 113/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0234 - val_accuracy: 0.9937\nEpoch 114/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0243 - val_accuracy: 0.9950\nEpoch 115/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0317 - val_accuracy: 0.9935\nEpoch 116/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0219 - val_accuracy: 0.9950\nEpoch 117/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0268 - val_accuracy: 0.9937\nEpoch 118/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0268 - val_accuracy: 0.9931\nEpoch 119/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0237 - val_accuracy: 0.9940\nEpoch 120/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0211 - val_accuracy: 0.9948\nEpoch 121/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0258 - val_accuracy: 0.9939\nEpoch 122/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0214 - val_accuracy: 0.9952\nEpoch 123/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0215 - val_accuracy: 0.9948\nEpoch 124/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0247 - val_accuracy: 0.9942\nEpoch 125/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0246 - val_accuracy: 0.9948\nEpoch 126/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0214 - val_accuracy: 0.9950\nEpoch 127/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0201 - val_accuracy: 0.9952\nEpoch 128/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0211 - val_accuracy: 0.9952\nEpoch 129/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0187 - val_accuracy: 0.9956\nEpoch 130/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0183 - val_accuracy: 0.9946\nEpoch 131/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0280 - val_accuracy: 0.9939\nEpoch 132/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0292 - val_accuracy: 0.9942\nEpoch 133/150\n280/280 [==============================] - 12s 45ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0214 - val_accuracy: 0.9950\nEpoch 134/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0296 - val_accuracy: 0.9936\nEpoch 135/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0241 - val_accuracy: 0.9945\nEpoch 136/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0209 - val_accuracy: 0.9942\nEpoch 137/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0214 - val_accuracy: 0.9946\nEpoch 138/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0269 - val_accuracy: 0.9927\nEpoch 139/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0216 - val_accuracy: 0.9948\nEpoch 140/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0215 - val_accuracy: 0.9946\nEpoch 141/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0213 - val_accuracy: 0.9945\nEpoch 142/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0205 - val_accuracy: 0.9949\nEpoch 143/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0274 - val_accuracy: 0.9946\nEpoch 144/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0264 - val_accuracy: 0.9937\nEpoch 145/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0222 - val_accuracy: 0.9948\nEpoch 146/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0266 - val_accuracy: 0.9944\nEpoch 147/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0234 - val_accuracy: 0.9942\nEpoch 148/150\n280/280 [==============================] - 12s 44ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0223 - val_accuracy: 0.9950\nEpoch 149/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0283 - val_accuracy: 0.9932\nEpoch 150/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0244 - val_accuracy: 0.9942\nEpoch 1/150\n280/280 [==============================] - 15s 50ms/step - loss: 0.4799 - accuracy: 0.9288 - val_loss: 0.3162 - val_accuracy: 0.9777\nEpoch 2/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.2234 - accuracy: 0.9796 - val_loss: 0.2109 - val_accuracy: 0.9752\nEpoch 3/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.1526 - accuracy: 0.9842 - val_loss: 0.2968 - val_accuracy: 0.9358\nEpoch 4/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.1169 - accuracy: 0.9857 - val_loss: 0.0939 - val_accuracy: 0.9892\nEpoch 5/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0933 - accuracy: 0.9872 - val_loss: 0.0620 - val_accuracy: 0.9910\nEpoch 6/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0785 - accuracy: 0.9878 - val_loss: 0.0512 - val_accuracy: 0.9919\nEpoch 7/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0656 - accuracy: 0.9894 - val_loss: 0.0796 - val_accuracy: 0.9874\nEpoch 8/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0585 - accuracy: 0.9901 - val_loss: 0.0593 - val_accuracy: 0.9929\nEpoch 9/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0532 - accuracy: 0.9899 - val_loss: 0.0358 - val_accuracy: 0.9933\nEpoch 10/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0489 - accuracy: 0.9904 - val_loss: 0.0390 - val_accuracy: 0.9907\nEpoch 11/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0456 - accuracy: 0.9909 - val_loss: 0.0380 - val_accuracy: 0.9926\nEpoch 12/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0391 - accuracy: 0.9916 - val_loss: 0.0555 - val_accuracy: 0.9875\nEpoch 13/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0411 - accuracy: 0.9908 - val_loss: 0.0509 - val_accuracy: 0.9886\nEpoch 14/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0357 - accuracy: 0.9917 - val_loss: 0.0290 - val_accuracy: 0.9925\nEpoch 15/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0345 - accuracy: 0.9923 - val_loss: 0.0345 - val_accuracy: 0.9912\nEpoch 16/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0313 - accuracy: 0.9925 - val_loss: 0.0333 - val_accuracy: 0.9921\nEpoch 17/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0298 - accuracy: 0.9931 - val_loss: 0.0249 - val_accuracy: 0.9931\nEpoch 18/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0304 - accuracy: 0.9928 - val_loss: 0.0233 - val_accuracy: 0.9940\nEpoch 19/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0263 - accuracy: 0.9933 - val_loss: 0.0268 - val_accuracy: 0.9936\nEpoch 20/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0252 - accuracy: 0.9937 - val_loss: 0.0293 - val_accuracy: 0.9932\nEpoch 21/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0239 - accuracy: 0.9940 - val_loss: 0.0223 - val_accuracy: 0.9943\nEpoch 22/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0243 - accuracy: 0.9935 - val_loss: 0.0271 - val_accuracy: 0.9929\nEpoch 23/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0228 - accuracy: 0.9937 - val_loss: 0.0341 - val_accuracy: 0.9913\nEpoch 24/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0222 - accuracy: 0.9937 - val_loss: 0.0206 - val_accuracy: 0.9939\nEpoch 25/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0198 - accuracy: 0.9946 - val_loss: 0.0236 - val_accuracy: 0.9948\nEpoch 26/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0197 - accuracy: 0.9948 - val_loss: 0.0238 - val_accuracy: 0.9932\nEpoch 27/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0202 - accuracy: 0.9942 - val_loss: 0.0208 - val_accuracy: 0.9939\nEpoch 28/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.0258 - val_accuracy: 0.9927\nEpoch 29/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0185 - accuracy: 0.9947 - val_loss: 0.0208 - val_accuracy: 0.9939\nEpoch 30/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.0237 - val_accuracy: 0.9933\nEpoch 31/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0151 - accuracy: 0.9960 - val_loss: 0.0207 - val_accuracy: 0.9944\nEpoch 32/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 0.0229 - val_accuracy: 0.9944\nEpoch 33/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0160 - accuracy: 0.9955 - val_loss: 0.0193 - val_accuracy: 0.9938\nEpoch 34/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0155 - accuracy: 0.9955 - val_loss: 0.0191 - val_accuracy: 0.9951\nEpoch 35/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 0.0195 - val_accuracy: 0.9943\nEpoch 36/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.0191 - val_accuracy: 0.9948\nEpoch 37/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0211 - val_accuracy: 0.9944\nEpoch 38/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.0203 - val_accuracy: 0.9950\nEpoch 39/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0121 - accuracy: 0.9966 - val_loss: 0.0229 - val_accuracy: 0.9939\nEpoch 40/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0127 - accuracy: 0.9965 - val_loss: 0.0207 - val_accuracy: 0.9944\nEpoch 41/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0121 - accuracy: 0.9966 - val_loss: 0.0227 - val_accuracy: 0.9944\nEpoch 42/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.0238 - val_accuracy: 0.9935\nEpoch 43/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0258 - val_accuracy: 0.9931\nEpoch 44/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0257 - val_accuracy: 0.9924\nEpoch 45/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 0.0261 - val_accuracy: 0.9931\nEpoch 46/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0263 - val_accuracy: 0.9935\nEpoch 47/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.0229 - val_accuracy: 0.9937\nEpoch 48/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0114 - accuracy: 0.9965 - val_loss: 0.0216 - val_accuracy: 0.9940\nEpoch 49/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0189 - val_accuracy: 0.9948\nEpoch 50/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.0270 - val_accuracy: 0.9929\nEpoch 51/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 0.0201 - val_accuracy: 0.9946\nEpoch 52/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0194 - val_accuracy: 0.9949\nEpoch 53/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0221 - val_accuracy: 0.9939\nEpoch 54/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.0230 - val_accuracy: 0.9945\nEpoch 55/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0102 - accuracy: 0.9970 - val_loss: 0.0259 - val_accuracy: 0.9931\nEpoch 56/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.0190 - val_accuracy: 0.9946\nEpoch 57/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0096 - accuracy: 0.9968 - val_loss: 0.0205 - val_accuracy: 0.9939\nEpoch 58/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0208 - val_accuracy: 0.9940\nEpoch 59/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0191 - val_accuracy: 0.9950\nEpoch 60/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0200 - val_accuracy: 0.9949\nEpoch 61/150\n280/280 [==============================] - 12s 45ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.0217 - val_accuracy: 0.9940\nEpoch 62/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0208 - val_accuracy: 0.9945\nEpoch 63/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0312 - val_accuracy: 0.9915\nEpoch 64/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0204 - val_accuracy: 0.9944\nEpoch 65/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0215 - val_accuracy: 0.9944\nEpoch 66/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0199 - val_accuracy: 0.9948\nEpoch 67/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0361 - val_accuracy: 0.9913\nEpoch 68/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.0197 - val_accuracy: 0.9948\nEpoch 69/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.0200 - val_accuracy: 0.9944\nEpoch 70/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.0174 - val_accuracy: 0.9950\nEpoch 71/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0203 - val_accuracy: 0.9945\nEpoch 72/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0200 - val_accuracy: 0.9939\nEpoch 73/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0200 - val_accuracy: 0.9946\nEpoch 74/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0242 - val_accuracy: 0.9936\nEpoch 75/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0224 - val_accuracy: 0.9939\nEpoch 76/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0224 - val_accuracy: 0.9943\nEpoch 77/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0190 - val_accuracy: 0.9954\nEpoch 78/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0229 - val_accuracy: 0.9932\nEpoch 79/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0185 - val_accuracy: 0.9948\nEpoch 80/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0205 - val_accuracy: 0.9949\nEpoch 81/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0203 - val_accuracy: 0.9948\nEpoch 82/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0216 - val_accuracy: 0.9931\nEpoch 83/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0207 - val_accuracy: 0.9946\nEpoch 84/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0199 - val_accuracy: 0.9950\nEpoch 85/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0202 - val_accuracy: 0.9945\nEpoch 86/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0252 - val_accuracy: 0.9924\nEpoch 87/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0215 - val_accuracy: 0.9944\nEpoch 88/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0220 - val_accuracy: 0.9935\nEpoch 89/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0188 - val_accuracy: 0.9951\nEpoch 90/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0229 - val_accuracy: 0.9939\nEpoch 91/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0241 - val_accuracy: 0.9940\nEpoch 92/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0229 - val_accuracy: 0.9942\nEpoch 93/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0211 - val_accuracy: 0.9942\nEpoch 94/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0215 - val_accuracy: 0.9946\nEpoch 95/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0220 - val_accuracy: 0.9957\nEpoch 96/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0224 - val_accuracy: 0.9945\nEpoch 97/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0221 - val_accuracy: 0.9951\nEpoch 98/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0196 - val_accuracy: 0.9951\nEpoch 99/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0197 - val_accuracy: 0.9951\nEpoch 100/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0240 - val_accuracy: 0.9943\nEpoch 101/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0223 - val_accuracy: 0.9936\nEpoch 102/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0199 - val_accuracy: 0.9954\nEpoch 103/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0222 - val_accuracy: 0.9943\nEpoch 104/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0216 - val_accuracy: 0.9943\nEpoch 105/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.0248 - val_accuracy: 0.9939\nEpoch 106/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0266 - val_accuracy: 0.9939\nEpoch 107/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0260 - val_accuracy: 0.9937\nEpoch 108/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0191 - val_accuracy: 0.9951\nEpoch 109/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0220 - val_accuracy: 0.9940\nEpoch 110/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0212 - val_accuracy: 0.9943\nEpoch 111/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0199 - val_accuracy: 0.9946\nEpoch 112/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0192 - val_accuracy: 0.9946\nEpoch 113/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0196 - val_accuracy: 0.9952\nEpoch 114/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0205 - val_accuracy: 0.9942\nEpoch 115/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0210 - val_accuracy: 0.9949\nEpoch 116/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0209 - val_accuracy: 0.9949\nEpoch 117/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0207 - val_accuracy: 0.9954\nEpoch 118/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.0235 - val_accuracy: 0.9940\nEpoch 119/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0209 - val_accuracy: 0.9949\nEpoch 120/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0222 - val_accuracy: 0.9945\nEpoch 121/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0227 - val_accuracy: 0.9946\nEpoch 122/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0227 - val_accuracy: 0.9950\nEpoch 123/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0214 - val_accuracy: 0.9945\nEpoch 124/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0202 - val_accuracy: 0.9946\nEpoch 125/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0231 - val_accuracy: 0.9937\nEpoch 126/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0217 - val_accuracy: 0.9950\nEpoch 127/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0199 - val_accuracy: 0.9949\nEpoch 128/150\n280/280 [==============================] - 14s 49ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0259 - val_accuracy: 0.9944\nEpoch 129/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0328 - val_accuracy: 0.9926\nEpoch 130/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0214 - val_accuracy: 0.9949\nEpoch 131/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0258 - val_accuracy: 0.9938\nEpoch 132/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0231 - val_accuracy: 0.9946\nEpoch 133/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0230 - val_accuracy: 0.9939\nEpoch 134/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0238 - val_accuracy: 0.9946\nEpoch 135/150\n280/280 [==============================] - 13s 47ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0258 - val_accuracy: 0.9942\nEpoch 136/150\n280/280 [==============================] - 14s 48ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0216 - val_accuracy: 0.9951\nEpoch 137/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0274 - val_accuracy: 0.9944\nEpoch 138/150\n280/280 [==============================] - 14s 50ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0265 - val_accuracy: 0.9942\nEpoch 139/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0228 - val_accuracy: 0.9938\nEpoch 140/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.0212 - val_accuracy: 0.9948\nEpoch 141/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0186 - val_accuracy: 0.9951\nEpoch 142/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0212 - val_accuracy: 0.9949\nEpoch 143/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0259 - val_accuracy: 0.9944\nEpoch 144/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0247 - val_accuracy: 0.9950\nEpoch 145/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0201 - val_accuracy: 0.9946\nEpoch 146/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0185 - val_accuracy: 0.9951\nEpoch 147/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0241 - val_accuracy: 0.9936\nEpoch 148/150\n280/280 [==============================] - 13s 46ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0269 - val_accuracy: 0.9944\nEpoch 149/150\n280/280 [==============================] - 13s 45ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0219 - val_accuracy: 0.9946\nEpoch 150/150\n280/280 [==============================] - 13s 48ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.0263 - val_accuracy: 0.9933\n","output_type":"stream"}]},{"cell_type":"code","source":"average_prediction_vectors=(prediction_vectors3+prediction_vectors5+prediction_vectors7)/3.\npredictions_final=np.argmax(average_prediction_vectors, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:16:36.052489Z","iopub.execute_input":"2022-03-01T19:16:36.053422Z","iopub.status.idle":"2022-03-01T19:16:36.060816Z","shell.execute_reply.started":"2022-03-01T19:16:36.053387Z","shell.execute_reply":"2022-03-01T19:16:36.059208Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Read sample_submission.csv in dataframe sub\nsub = pd.read_csv('/kaggle/input/digit-recognizer/sample_submission.csv')\n\n#Overwrite labels in dataframe sub\nsub[\"Label\"] = predictions_final\n\n#Write updated dataframes as submission.csv\nsub.to_csv('submission.csv',index=False)\n#sub.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T19:16:37.466875Z","iopub.execute_input":"2022-03-01T19:16:37.467537Z","iopub.status.idle":"2022-03-01T19:16:37.526442Z","shell.execute_reply.started":"2022-03-01T19:16:37.467491Z","shell.execute_reply":"2022-03-01T19:16:37.525508Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}